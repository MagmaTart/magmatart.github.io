---
layout: article
title:  "딥 러닝 최적화 알고리즘 정리"
date:   2018-01-01 12:00:00 -0400
modify_date: 2018-01-01 12:00:00 -0400
tags:
- Machine Learning
- Deep Learning
category: 
- Development
use_math: true
---

SGD, Momentum, NAG, AdaGrad, RMSProp, Adam

<!--more-->
-----
지금까지 공부하면서 대부분의 경우에 최적화(Optimization, 손실 최소화) 알고리즘으로 Gradient Descent 알고리즘을 사용했었다. 하지만 저번 MNIST 분류를 구현하면서 Adam이라는 새로운 Optimization 알고리즘을 사용했다. 정확히 어떤 원리로 작동하는 알고리즘인지 모르는 채로 사용했었는데, 이번 기회에 Optimization 알고리즘들을 한번 정리해보려고 한다.

### SGD (Stochastic Gradient Descent)
__확률적 경사 하강법__ 이라고 불리는 알고리즘이다. 알고리즘의 동작 방식은 Gradient Descent와 동일하다. 아래와 같이, 현재 가중치 매개 변수의 값에서의 손실 함수의 미분값을 이용하여 가중치를 업데이트한다.

$$W \leftarrow W - \eta \frac {\partial L}{\partial W}$$

이 경사 하강법을 모든 Training Set을 기반으로 수행하는 것을 __Batch Gradient Descent__ 이라고 한다. 이 방법은 전체 학습 데이터에 대한 손실 함수를 계산하여 가중치를 갱신할 수 있으므로 정확한 학습이 가능하지만, 데이터가 많아질 수록 너무 많은 연산이 필요하게 된다.

그래서 모든 Training Set이 아닌, 세트의 일부만을 가지고 Loss Function을 계산한 후 Gradient Descent 알고리즘을 적용하여 가중치를 갱신하는 방법이 바로 __Stochastic Gradient Descent__ 이다. 그리고 이 조그마한 세트의 일부를 __Mini Batch__ 라고 한다. 배치 학습을 수행할 때의 그것과 의미가 같다.

SGD를 사용하면 그때 그때 훨씬 적은 데이터를 이용해 최적화를 진행하므로 계산량이 크지 않아 속도가 비교적으로 빠르다. 최적화의 결과 또한 Batch Gradient Descent와 비슷한 결과로 수렴된다. 오히려 더 좋은 방향으로 수렴할 가능성도 생긴다.

### Momentum
SGD는 Local Minimum에 매우 취약한 알고리즘이다. 아래의 그림은 여러 최적화 알고리즘들의 수렴 모습인데, SGD는 결국 Local Minimum에 빠지고 마는 모습을 보여 준다.

<p align="center"><image width="400" src="/assets/posts/images/36/SaddlePoint.gif"/></p>

SGD의 문제점을 해결하기 위해 수많은 기법들이 고안되었고, 그 중 하나가 __Momentum__ 이다.

Momentum의 사전적 뜻은 '탄력, 추진력' 등이다. Momentum 기법은 사전적 의미 그대로, Gradient Descent에 의해 가중치가 변화하는 방향에 탄력 또는 추진력을 주는 기법이다. 수식으로 먼저 보자.

$$v \leftarrow \alpha v - \eta \frac {\partial L}{\partial W}$$

$$W \leftarrow W + v$$

$$v$$ 물리에서 말하는 '속도' 라고 생각하면 편하다. 현재 Gradient Descent 알고리즘에 의해 계산된 이동 방향과 이동량을 이용해 속도를 변화시킨다. 그리고, 가중치를 그 
'속도'만큼 변화시켜주고 있는 것을 볼 수 있다. 보통 $$\alpha$$에는 0.9등의 값을 준다고 한다.

이렇게 가중치를 업데이트하게 되면, 가중치는 현재 기울기($$\frac {\partial L}{\partial W}$$)의 방향으로 __가속__ 되어 업데이트된다. 따라서 최적값과 멀리 떨어져있을 때는 최적화에 가속이 붙고, $$\alpha v$$ 항이 이전의 속도를 기억하므로 최적화 이상으로 업데이트되어 다시 값이 튀었을 때 빠르게 잡아줄 수도 있다. 또한 기울기에 의한 갱신량이 0일 때, $$\alpha v$$ 항에 의해 이동하는 거리가 생기므로, Local Minimum에 빠질 확률을 줄여줄 수 있다.

아래의 그림은 Momentum을 적용한 최적화와 그렇지 않은 최적화의 가중치 갱신 경로를 보여주고 있다.

<p align="center"><image width="550" src="/assets/posts/images/36/momentum.png"/></p>

### NAG (Nesterov Accelerated Gradient)
NAG는 Momentum을 약간 변형한 기법이다. Momentum이 기울기와 현재의 속도를 동시에 고려해서 다음 이동 방향과 속도를 결정한다면, NAG는 일단 현재 속도로 이동한 후 기울기를 고려해서 다음 이동 방향을 정한다. 식으로 보면 다음과 같다.

$$v \leftarrow \alpha v - \eta ( \frac {\partial L}{\partial W} + \alpha v)$$

$$W \leftarrow W + v$$

먼저 $$\alpha v$$의 속도로 이동한 후, 그 결과의 기울기를 이용하여 속도를 갱신하고 있다. 이를 그림으로 보면 다음과 같을 것이다. 그림은 CS231n의 슬라이드에서 가져왔다.

<p align="center"><image width="650" src="/assets/posts/images/36/NAG.PNG"/></p>

NAG는 일반 Momentum보다 평균적으로 더 좋은 성능을 보인다. 일반 Momentum 방식은 기울기에 따라 이동을 한 후 속도에 따라 다시 이동하는 방식이지만, NAG는 속도에 따라 미리 이동한 후 기울기를 고려하여 속도를 변화시키기 때문에, Momentum의 속도를 이용한 빠른 이동이라는 이점을 살리면서 더욱 안정적인 이동을 할 수 있게 된다.

### AdaGrad (Adaptive Gradient)
신경망의 학습에는 학습률(learning rate)이 중요하다. 학습률 값이 너무 작으면 학습 시간이 너무 길어지고, 너무 크면 학습이 제대로 이뤄질 수 없기 떄문이다. 하이퍼 파라미터인 이 학습률을 효과적으로 정하는 기법 중 하나가 __학습률 감소__ 이다. 학습이 반복될수록 학습률을 점차 낮춰가는 방법을 말한다. 이 방법은 실제 신경망 학습에 자주 쓰인다.

AdaGrad가 학습률 감소를 도입한 기본적인 매개변수 최적화 알고리즘이다. 간단하게, 가중치 매개변수 전체의 학습률을 일괄적으로 낮추는 것이다, AdaGrad는 이를 구현하여, 가중치 매개변수의 각각 원소에 맞는 맞춤형 학습률 값을 만들어 준다. AdaGrad를 수식으로 보면 다음과 같다.

$$h \leftarrow h + \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}$$

$$W \leftarrow W - \eta \frac{1}{\sqrt{h}} \odot \frac{\partial L}{\partial W}$$

$$h$$는 일종의 메모리같은 역할을 하며, 이전 기울기들을 제곱한 값을 계속 더해 나간다. 이 때 제곱은 원소간의 제곱이다(Element-wise). 그리고 실제 가중치 매개변수를 수정할 때, 학습률에 $$\frac {1}{\sqrt{h}}$$를 곱해준 값을 실제 학습률로 사용한다. 이 때 학습률은 행렬 형태가 되므로, 기울기 값과 연산하였을 때 모든 원소가 동일한 학습률을 적용받지 않고, 각 원소별로 최적화된 학습률을 적용받을 수 있게 된다.

$$h$$의 변화에 따라 각 원소마다 변화량이 달라지는데, 이전 기울기 $$\frac {\partial L}{\partial W}$$에서 어떤 원소의 변화가 컸을 경우 $$h$$에서 해당 원소의 값이 커져 해당 매개변수의 학습률이 더 크게 감소하고, 원소의 기울기 변화가 작았을 경우에는 해당 매개변수의 학습률이 더 작게 감소한다.

AdaGrad는 좋은 알고리즘이지만 문제점을 가지고 있다. $$h$$의 값이 계속 커지기만 하므로, 언젠가는 학습률이 0에 수렴해 학습이 진행되지 않는 상황이 발생할 수 있다는 것이다.

### RMSProp
AdaGrad의 위와 같은 문제를 해결하기 위하여 만들어진 알고리즘이 __RMSProp__ 이다. RMSProp의 아이디어는, 먼 과거의 기울기를 서서히 잊고 최근의 새로운 기울기 정보를 학습률 갱신에 크게 반영하자는 것입니다. 그리고 이 과정에서, __지수이동평균(EMA)__ 을 사용한다.

지수이동평균을 잠깐 알아보고 가자. 최신의 정보(값)을 $$x_n$$, 과거의 정보(값)들의 종합을 $$s_n$$, 0과 1 사이의 값을 가지는 계수를 $$\alpha$$로 놓을 때, 업데이트되는 다음의 정보 $$s_{n+1}$$는 다음과 같이 정의된다.

$$s_{n+1} = \alpha x_n + (1-\alpha)s_n$$

$$\alpha$$가 0에 가까울 수록 과거의 값만 반영하고, 1에 가까울수록 최신의 값만 반영하게 된다. 보통 0.8이나 0.9에 가까운 값을 사용하는 것 같다.

값이 업데이트될수록 과거의 값들은 $$(1 - \alpha)$$라는 계수의 영향을 계속 누적해서 받게 되어 영향이 점점 줄어들게 된다. 하지만 영향이 아예 사라지지는 않는다

이 방법을 기울기와 학습률에 적용하는 것이 RMSProp이다. 아래의 식은, RMSProp의 작동 방식을 보여준다.

$$h \leftarrow \gamma h + (1-\gamma) (\frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W})$$

$$W \leftarrow W - \eta \frac{1}{\sqrt{h}} \odot \frac{\partial L}{\partial W}$$

학습률의 갱신에 지수이동평균을 적용하고 있다. 지수이동평균의 성질에 의해 과거의 값들은 서서히 잊혀지고, 새로 갱신된 기울기의 제곱 값이 상대적으로 크게 반영되고 있는 것을 볼 수 있다. RMSProp는 이 방법을 사용하여 $$h$$의 값이 무한정으로 커지는 문제를 해결했다.

### Adam
Adam은 직관적으로 보면 Momentum과 RMSProp의 특징을 합쳐놓은 것 같은 알고리즘이다. 매개변수의 탐색 속도를 가속시켜주는 Momentum의 특징과, 매개변수의 각 원소에 알맞는 학습률을 만들어주는 RMSProp의 특징을 합친 알고리즘이다. 지수이동평균을 RMSProp에 해당하는 변수에만 사용하는 것이 아니라 Momentum의 '속도' 역할 변수를 업데이트할때도 사용하는 것이 핵심이다.

수식 쓰기 귀찮아서 논문 내용을 그대로 가져왔다.

<p align="center"><image width="650" src="/assets/posts/images/36/adam.png"/></p>

앞서 얘기했던 지수이동평균을 '모먼트'라 하고, Adam에서는 두 개의 모먼트를 사용해 Momentum($$m_t$$)과 RMSProp($$v_t$$) 알고리즘에서 파악하는 것과 동일한 파라미터 변화량을 계산한다. 그리고 두 개의 모먼트 값을 모두 반영하여 파라미터를 업데이트하게 된다.

### Conclusion
지금까지 다양한 가중치 매개변수 최적화 알고리즘들을 알아보았는데, 대부분 이전의 알고리즘이 가지고 있던 문제를 해결하면서 개발된 알고리즘이라는 것을 알 수 있었다.

또, 적지 않은 상황에서 Adam Optimizer가 크게 활용된다는 사실도 알 수 있었다.