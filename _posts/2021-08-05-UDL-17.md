---
layout: article
title:  "[딥러닝 알아듣기] 3.8. 분류 문제의 다양한 평가 지표"
date:   2021-08-05 12:00:00 -0400
modify_date: 2021-08-05 12:00:00 -0400
tags:
- 딥러닝 알아듣기
- 딥러닝
- Machine Learning
- Deep Learning
category: 
- series
use_math: true
---

"딥러닝 알아듣기" 시리즈는 딥러닝의 기초 지식을 저만의 방식으로 쉽게 풀어내는 시리즈입니다.
이번 챕터에서는 분류 문제에서 모델의 성능을 평가하기 위해 사용되는 대표적인 지표들에 대해 알아봅니다.

<!--more-->
-----
앞서 이중 분류와 다중 분류 모델을 각각 학습해 보았다. 학습한 모델이 의도한 대로 잘 동작하는 것만을 확인하기 위해서, 평가 지표를 따로 정의하지 않고 단순한 분류 정확도(Accuracy) 로만 모델의 성능을 평가했다. 이번 챕터에서는 단순 정확도에서 벗어나, 상황에 따라 분류 모델의 성능을 효과적으로 평가하기 위한 다양한 평가 지표들을 알아본다. 

## 3.8.1. 이중 분류 문제의 평가 지표
분류 정확도는 직관적으로 생각해낼 수 있는 분류 모델의 가장 단순한 평가 지표이다. 그러나 현실 세계의 분류 문제를 풀어내기 위한 딥 러닝 모델을 평가할 때, 대부분의 경우 정확도는 좋은 평가 지표가 아니다.

평가 지표를 단순 정확도로 설정했을 때의 맹점을 파악하기 위해, 하나의 이중 분류 문제 상황을 가정해보자. 문제 상황은 예시를 위해 연출된 상황이며, 문제 상황에서 수집된 데이터 또한 임의로 만들어진 데이터이다. 현실 세계와는 아무런 연관도 없다.

어떤 병원에서 데이터를 기반으로 환자의 심장병 발생 가능성을 예측하고자 한다. 심장병이 발생했던 환자와 그렇지 않은 환자들의 신체 상태 데이터를 수집해서, 발병 가능성이 높은 환자들을 집중 관리하기 위함이다. 이를 위해 기존 환자들과 일반인들의 신체 정보 데이터를 수집한 후 다음과 같이 필요한 특징들만 정제했다. 에제 상황이니 단순하게 나이, 허리둘레, 수축기 혈압의 세 가지 특징 값만을 취했다. 정답 레이블이 0이면 해당 인물의 심장병의 발병 가능성이 없다는 뜻이고, 1이면 발병 가능성이 존재한다는 뜻이다.

<center><img src="/assets/posts/images/UDL17/figure2.png" width=400></center>

하지만 이렇게 데이터셋을 만들고 나니 심각한 문제가 보였다. 각각의 인물이 실제로 심장병을 앓았는지의 여부로 발병 가능성을 레이블링했다. 그러나 정상인보다 발병한 환자의 수가 현저히 적어 데이터의 균형이 심하게 깨지는 문제가 발생했다. 1만명의 데이터를 수집했지만 그 중 발병 가능성이 있음으로(1) 레이블링된 인물은 1천명에 불과했다. 9:1의 극단적인 비율을 가지고 있는 것이다.

모은 데이터를 각각 레이블의 비율을 맞추어 학습 데이터셋 7, 평가 데이터셋 3 비율로 나누고 분류 모델을 학습시켰다. 이 때, 학습한 모델을 단순 정확도만으로 평가하면 큰 문제가 발생한다. 평가 데이터셋 또한 발병 가능성이 없는 사람과 있는 사람의 데이터를 9:1의 극단적인 비율로 가지고 있기 때문이다. 일례로, 학습을 하나도 시키지 않아 모든 데이터를 발병 가능성이 없다고 분류하는 모델을 사용해도 __90%__ 의 높은 정확도를 보인다. 이렇듯 클래스 별 균형이 맞지 않는 데이터에 대한 모델을 단순 정확도로 평가하면 합리적인 성능 평가를 할 수 없다. 평가 데이터셋에서 90%로 비교적 높은 정확도를 보였다고 해도, 실제로 병원에서 환자의 판단을 위해 절대 사용할 수 없을 것이다. 모든 환자를 발병 가능성 없다고 판단하는 모델을 어떻게 믿고 사용할 것인가?

단순 정확도 지표를 최대한으로 신뢰하기 위해서는, 평가 데이터셋이 모든 클래스에 대한 데이터를 비슷한 비율로 가지고 있어야 한다. 그러나 현실에서 마구 모은 데이터에 클래스 별 균형을 바라는 것은 크나큰 사치다. 실제로 딥 러닝을 적용하고자 하는 대부분의 상황에서는 데이터셋의 클래스 불균형이 필연적으로 존재한다. 단순 정확도에서 벗어나 클래스별 불균형 또한 고려된 더욱 정밀한 성능 평가 지표가 필요한 이유이다.

이제 이중 분류 모델의 정밀한 성능 평가 지표를 알아보도록 하자. 여러 분야에서 가장 대중적으로 사용되는 평가 지표이므로, 잘 기억해두면 향후 딥 러닝 공부가 한 층 수월해질 것이다.

### 혼동 행렬 (Confusion Matrix)
주어진 데이터를 이중 분류하는 딥 러닝 모델을 하나 떠올려보자. 앞에서 이야기했던 심장병 발병 가능성 판단 모델을 하나의 예로 들 수 있겠다. 환자의 데이터를 모델에 입력해서 분류 결과를 얻었을 때, 정답 레이블의 상태에 따라 다음의 네 가지 상황으로 분류 결과를 나누어볼 수 있다.

- 참 긍정(True Positive, TP) : 발병 가능성이 __있다고(긍정, Positive) 옳게 판단(True)__ 한 경우 
- 거짓 긍정(False Positive, FP) : 발병 가능성이 __있다고(긍정, Positive) 잘못 판단(False)__ 한 경우
- 참 부정(True Negative, TN) : 발병 가능성이 __없다고(부정, Negative) 옳게 판단(True)__ 한 경우
- 거짓 부정(False Negative, FN) : 발병 가능성이 __없다고(부정, Negative) 잘못 판단(False)__ 한 경우

데이터의 정답 클래스가 긍정인지 부정인지, 그리고 모델의 판단이 맞는지 여부에 따라 분류 결과를 네 가지 상황으로 나누었다. 참과 거짓은 모델의 판단이 맞았는지의 여부를 이야기하는 것이다. 참은 모델이 분류에 성공한 것이며, 거짓은 분류에 실패한 상황이다. 또한 긍정과 부정은 1과 0의 정답 클래스로 생각하면 된다. 우리 모델의 경우 발병 가능성이 있는 데이터가 긍정 클래스이고, 발병 가능성이 없는 데이터가 부정 클래스의 데이터이다.

평가 데이터셋 전체에 대한 모델의 판단 결과를 구한 후, 각각의 결과가 어디에 속하는지 갯수를 세어볼 수 있다. 직접 학습한 발병 가능성 예측 모델의 평가 데이터셋에서 추론 결과가 다음과 같이 정리되었다. 이러한 형태로 나타낸 이중 분류 모델의 추론 결과를 __혼동 행렬(Confusion Matrix)__ 라고 부른다.

<center><img src="/assets/posts/images/UDL17/figure1.png" width=600></center>

### 혼동 행렬 관련 지표
이제부터 앞에서 구한 혼동 행렬으로 여러 가지 성능 평가 지표를 정의해보자. 

#### 분류 정확도 (Accuracy)
가장 먼저 계속 사용해오던 분류 정확도(Accuracy)를 혼동 행렬을 통해 다시 정의해볼 수 있다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BAccuracy%7D%20%3D%20%5Cfrac%7B%5Ctext%7BTP%7D%20&plus;%20%5Ctext%7BTN%7D%7D%7B%5Ctext%7BTP%7D%20&plus;%20%5Ctext%7BFP%7D%20&plus;%20%5Ctext%7BTN%7D%20&plus;%20%5Ctext%7BFN%7D%7D" width=300></center>
.
<center><img src="/assets/posts/images/UDL17/figure3.png" width=300></center>

앞에서 구한 우리 모델의 분류 정확도는 다음과 같이 계산된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BAccuracy%7D%20%3D%20%5Cfrac%7B236%20&plus;%202690%7D%7B236%20&plus;%2010%20&plus;%202690%20&plus;%2064%7D%20%3D%200.9753" width=400></center>

#### 에러율 (Error Rate)
다음으로 생각해볼 수 있는 평가 지표는 __에러율(Error Rate)__ 이다. 에러율은 정확도의 반대 개념으로, 전체 데이터 중 잘못 분류한 비율이 어느 정도인지를 나타내는 지표이다. 다음과 같이 정의된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BError%20Rate%7D%20%3D%20%5Cfrac%7B%5Ctext%7BFP%7D%20&plus;%20%5Ctext%7BFN%7D%7D%7B%5Ctext%7BTP%7D%20&plus;%20%5Ctext%7BFP%7D%20&plus;%20%5Ctext%7BTN%7D%20&plus;%20%5Ctext%7BFN%7D%7D" width=300></center>
.
<center><img src="/assets/posts/images/UDL17/figure4.png" width=300></center>

정의에서 알 수 있듯이, 에러율과 정확도를 더하면 1이다. 즉, $$\text{Error Rate} = 1 - \text{Accuracy}$$이다.

우리 모델의 분류 에러율은 다음과 같이 계산된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BError%20Rate%7D%20%3D%20%5Cfrac%7B10%20&plus;%2064%7D%7B236%20&plus;%2010%20&plus;%202690%20&plus;%2064%7D%20%3D%200.0247" width=400></center>

#### TNR (True Negative Rate), 특이도 (Specificity)
다음은 __TNR(True Negative Rate)__ 이다. __특이도(Specificity)__ 라고도 부른다. TNR은 실제로 부정 클래스인 데이터를 부정 클래스라고 분류에 성공한 비율을 의미한다. 다음과 같이 정의된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BTNR%7D%20%3D%20%5Cfrac%7B%5Ctext%7BTN%7D%7D%7B%5Ctext%7BTN%7D%20&plus;%20%5Ctext%7BFP%7D%7D" width=150></center>
.
<center><img src="/assets/posts/images/UDL17/figure5.png" width=300></center>

부정 클래스라고 옳게 판단한 것들과(TN) 부정 클래스인데 긍정이라고 잘못 판단한 것들의(FP) 갯수를 모두 합하면, 레이블이 부정 클래스인 데이터의 전체 갯수를 구할 수 있다.

우리 모델의 TNR은 다음과 같이 계산된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BTNR%7D%20%3D%20%5Cfrac%7B%5Ctext%7B2690%7D%7D%7B%5Ctext%7B2690%7D%20&plus;%20%5Ctext%7B10%7D%7D%20%3D%200.9963" width=250></center>

데이터셋의 균형이 맞지 않고 부정 클래스로만 편향되어 있기 때문에, TNR이 높은 값을 가질 수밖에 없다.

#### FPR (False Positive Rate)
다음으로 살펴볼 지표는 __FPR(False Positive Rate)__ 이다. FPR은 실제로 부정 클래스인 데이터를 긍정 클래스로 잘못 분류한 비율을 의미한다. FPR이 높다면 모델이 대부분의 데이터들을 긍정 클래스로만 분류하고 있다고 이해할 수 있다. FPR은 다음과 같이 정의된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BFPR%7D%20%3D%20%5Cfrac%7B%5Ctext%7BFP%7D%7D%7B%5Ctext%7BTN%7D%20&plus;%20%5Ctext%7BFP%7D%7D" width=150></center>
.
<center><img src="/assets/posts/images/UDL17/figure6.png" width=300></center>

정확도와 에러율의 관계처럼, FPR과 TNR도 더하면 1이다. 둘 다 부정 클래스인 데이터들에 대한 지표이기 때문이다.

우리 모델의 FPR은 다음과 같이 계산된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BFPR%7D%20%3D%20%5Cfrac%7B%5Ctext%7B10%7D%7D%7B%5Ctext%7B2690%7D%20&plus;%20%5Ctext%7B10%7D%7D%20%3D%200.0037" width=250></center>

#### 재현율(Recall)
분류 정확도를 계산하는 식에서 주목해야 할 부분이 있다. 최종으로 계산된 정확도는 0.9887로 높은 수치를 기록했지만, 정작 중요한 심장병 발병 가능성이 있는 사람들에 대해서는 성능이 상대적으로 약함을 알 수 있다. 참 긍정(TP)와 거짓 부정(FN)의 경우가 실제로 심장병 발병 가능성이 있는 데이터에 대한 결과이다. 전체적인 분류 정확도는 매우 높지만, 실제 발병가능성이 있는 사람을 찾아내는 능력은 상대적으로 떨어진다는 이야기이다.

방금 생각한 평가 지표가 바로 __재현율(Recall)__ 이다. __TPR(True Positive Rate)__ 또는 __민감도(Sensitivity)__ 라고 부르기도 한다. 수식으로는 다음과 같이 나타내어진다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BRecall%7D%20%3D%20%5Cfrac%7B%5Ctext%7BTP%7D%7D%7B%5Ctext%7BTP%7D%20&plus;%20%5Ctext%7BFN%7D%7D" width="150"></center>
.
<center><img src="/assets/posts/images/UDL17/figure7.png" width=300></center>

우리 모델의 재현율은 다음과 같이 계산된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BRecall%7D%20%3D%20%5Cfrac%7B236%7D%7B236%20&plus;%2064%7D%20%3D%200.787" width=225></center>

모델의 분류 정확도는 매우 높았지만, 실제로 발병 가능성이 있는 환자들에 대한 판단 정확도가 상대적으로 부족한것으로 보인다. 여기서 분류 정확도가 높았던 이유가 수많은 참 부정(TN) 때문이었음을 알 수 있다. 학습과 평가 데이터셋에 부정 클래스(Negative)인 데이터 비율이 너무 크기 때문이다. 모델의 학습시에 발병 가능성이 없는 부정 클래스의 데이터만 잔뜩 학습하기 때문에, 부정 클래스의 데이터는 훨씬 쉽게 구분해낼 수 있다. 따라서 이런 경우 재현율 지표를 꼭 확인해보아야 한다. 분류 정확도가 아무리 높아도 정작 발병 가능성이 있는 환자를 찾아내지 못하면 모델이 아무런 의미가 없어질 것이다.

#### 정밀도(Precision)
방금 살펴본 재현율 지표는 모델의 합리적인 성능 평가에 매우 유용하지만, 단독으로 사용해서 모델을 평가하기에는 부족한 점이 있다. 만약 우리 모델이 모든 입력 데이터를 발병 가능성이 있다고 판단한다면 재현율은 1일 것이다. 재현율은 긍정 클래스인 데이터들에 대해서만 측정되는 지표이기 때문이다. 따라서 재현율만을 보고 모델의 성능을 평가할 수는 없고, 부정 클래스를 잘 골라내는지에 대한 성능도 동시에 측정할 수 있는 지표가 필요하다. __정밀도(Precision)__ 의 개념이 여기서 등장한다.

정밀도는 수식으로 다음과 같이 나타내어진다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BPrecision%7D%20%3D%20%5Cfrac%7B%5Ctext%7BTP%7D%7D%7B%5Ctext%7BTP%7D%20&plus;%20%5Ctext%7BFP%7D%7D" width=200></center>
.
<center><img src="/assets/posts/images/UDL17/figure8.png" width=300></center>

우리 모델의 정밀도는 다음과 같이 계산된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BPrecision%7D%20%3D%20%5Cfrac%7B236%7D%7B236%20&plus;%2010%7D%20%3D%200.959" width="250"></center>

모델이 발병 가능성이 있다고 한 사람들 중, 실제로 가능성이 있는 사람과 그렇지 않은 사람들이 있을 것이다. 정밀도는 모델이 긍정으로 분류한 데이터 중 실제로 긍정 클래스인 데이터의 비율을 구한다. 모든 데이터를 긍정으로 분류하는 모델의 재현율은 1이지만 정밀도는 0에 가깝게 된다. 거짓 긍정(FP)이 크게 늘어나기 때문이다.

정밀도 또한 성능 평가를 위한 단독 지표로 사용할 수는 없다. 지표의 계산시에 부정 클래스에 대한 검출 결과는 하나도 고려되지 않았기 때문에, 모델의 전체적인 성능 평가 결과로 보기 어렵다.

재현율과 정밀도는 비슷해보이지만 평가하고자 하는 상황이 다르다. 재현율은 긍정 클래스를 얼마나 __많이__ 잡아내는지를 보고자 하는 지표이고, 정밀도는 긍정 클래스들을 얼마나 __정확하게__ 잡아내는지를 보려는 지표이다. 따라서 재현율과 정밀도는 필연적으로 반비례에 가까운 관계를 가진다(정확히 반비례하지는 않는다).  긍정 클래스로 분류하는 비율이 늘어나 재현율이 커지면 자연스럽게 거짓 긍정(FP)도 늘어나 정밀도가 하락할 가능성이 크다. 반대로 긍정 클래스를 더욱 정확히 분류해 정밀도가 늘어날수록 긍정으로 분류하는 데이터의 갯수 자체가 줄어드므로 거짓 부정(FN)이 늘어나 재현율이 하락할 가능성이 있다.

#### F1 Score
이중 분류 모델의 성능 평가에 재현율과 정밀도를 각각 단독으로는 사용할 수 없다. 반드시 두 지표를 동시에 고려해서, 재현율과 정밀도가 동시에 최대한으로 높은 모델을 찾아내야 한다. 이에 따라 재현율과 정밀도를 동시에 판단하여 모델의 성능을 하나의 값으로 평가할 수 있는 지표가 필요하게 되었고, __F1 점수(F1 Score)__ 가 개발된다.

F1 점수는 재현율과 정밀도의 __조화 평균__ 이다. 수식으로는 다음과 같이 나타내어진다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20F_1%20%3D%202%20%5Ccdot%20%5Cfrac%7B%5Ctext%7BRecall%7D%20%5Ccdot%20%5Ctext%7BPrecision%7D%7D%7B%5Ctext%7BRecall%7D%20&plus;%20%5Ctext%7BPrecision%7D%7D" width=225></center>

재현율과 정밀도가 동시에 높을수록 F1 점수 또한 높은 값을 가진다. 단순 산술 평균을 사용했을 경우, 하나의 지표가 0이고 다른 하나가 1이라면 점수는 0.5였을 것이다. F1 점수는 두 지표를 동시에 비중있게 고려해야하기 때문에 조화평균을 사용하는 것이 적절하다.

우리 모델의 F1 Score는 다음과 같이 계산될 것이다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20F_1%20%3D%202%20%5Ccdot%20%5Cfrac%7B0.787%20%5Ccdot%200.959%7D%7B0.787%20&plus;%200.959%7D%20%3D%200.8645" width=275></center>

### 곡선을 통한 분류 임계값 탐색
지금까지 이중 분류 모델의 성능을 평가하면서, 가장 중요한데 고려되지 않은 점이 하나 있다. 바로 __분류 임계값의 변화__ 이다. 제일 처음 계산해본 단순 분류 정확도부터 재현율과 정밀도, F1 Score의 지표까지 모두 __임계값이 하나로 고정된 상황__ 에서의 성능만을 측정할 수 있는 지표이다. 지금까지는 모델의 출력이 0과 1 사이의 값으로 나오니 분류 임계값을 단순히 0.5로 맞추어 사용했다. 순전히 경험과 생각에 기반한 임계값 설정이다.

#### 정밀도-재현율 곡선
모델의 출력 값이 똑같아도 분류 임계값이 달라지면 분류 결과가 달라질 수 있다. 즉, 우리 모델의 임계값을 0.5가 아닌 다른 값으로 설정하면 혼동 행렬이 완전히 달라질 수 있다는 이야기이다. 임계값을 0.2로 변경하면 혼동 행렬이 다음과 같이 변화하고, 그에 따라 재현율과 정밀도 또한 변화한다.

<center><img src="/assets/posts/images/UDL17/figure13.png" width=600></center>

같은 모델이라도 분류 임계값 설정에 따라 평가 성능이 달라진다면, 당연히 최고로 성능이 좋게 평가되는 임계값을 찾아내야 한다. 방법은 단순하다. 0부터 1 사이로 임계값을 변화시켜가며 각 지점에서 모델의 재현율과 정밀도 지표를 모두 계산한다. 모든 지점 중 두 지표가 모두 필요한 만큼 높이 측정되는 최적의 임계값을 찾으면 된다.

이렇게 모든 임계값에 대해 모델의 재현율과 정밀도 지표를 한눈에 나타낸 곡선을 __정밀도-재현율 곡선(Precision-Recall Curve)__ 이라고 부른다. 학습한 발병 가능성 분류 모델의 Precision-Recall 커브는 다음과 같이 나타난다.

<center><img src="/assets/posts/images/UDL17/figure9.png" width=500></center>

정밀도-재현율 곡선을 처음 접하면, 두 지표의 조화 평균이 가장 높은 지점인 곡선의 중앙에 위치하는 임계값이 무조건 제일 좋은 지점이라고 생각하기 쉽다. 단순히 생각하면 틀린 말은 아니지만 모델의 문제 상황에 따라 정밀도와 재현율을 적당히 타협하여 임계값을 결정해야 한다. 우리가 학습한 심장병 발병 가능성 판단 모델의 경우를 생각해보자. 어떤 병원에서 이 모델을 실제 환자 상태 파악에 사용한다면, 심장병 발병 가능성이 있는 사람을 최대한 많이 찾아내는 것이 중요하다. 발병 가능성이 없는 사람을 있다고 잘못 판단하는 것보다 발병 가능성이 있는데 찾아내지 못하는 것이 훨씬 치명적이기 때문이다. 정밀도-재현율 곡선을 그려보았을 때 두 지표가 동시에 매우 높은 값을 보이도록 하는 임계값이 없다면, 정밀도를 약간 포기하더라도 재현율이 최대한 높아지는 분류 임계값을 사용해서 발병 가능성이 있는 환자를 최대한 많이 찾아내야 한다. 모델의 전체적인 성능과 문제 상황을 동시에 고려하여 분류 임계값을 신중히 설정해야 된다는 이야기이다. 이것이 정밀도-재현율 곡선을 사용하는 이유이다.

다음의 그림에서 초록색 점은 분류 임계값이 0.2인 점이고, 빨간 점은 분류 임계값이 0.8인 점이다. 분류 임계값이 낮아지면 긍정으로 분류하는 샘플의 수가 많아져 TP와 FP가 동시에 증가하므로 정밀도가 하락한다. 또한 동시에 재현율은 상승한다. 대부분의 샘플을 긍정으로 분류하게 되어 TP의 갯수가 자연스럽게 늘어나기 때문이다. 따라서 문제 상황에 따라 정밀도와 재현율의 판단 비중을 설정해서 알맞은 분류 임계값을 설정해야 한다.

<center><img src="/assets/posts/images/UDL17//figure10.png" width=500></center>

#### ROC 곡선
이와 비슷한 용도로 __ROC 곡선__ 이라는 녀석도 많이 쓰인다. 영어로는 'Receiver Operation Characteristic Curve', 우리말로는 '수신자 조작 특성 곡선' 이라는 다소 어려운 말로 정의되어 있다. 용도는 정밀도-재현율 곡선과 동일하게 최적의 파라미터를 찾는 것이지만, TPR과 FPR 지표를 사용해서 모델의 성능을 나타낸다는 점이 다르다. __TPR은 재현율과 동일한 지표__ 이다. 우리 모델의 ROC 곡선을 그리면 다음과 같이 나타난다.

<center><img src="/assets/posts/images/UDL17/figure11.png" width=500></center>

모델의 분류 임계값이 낮아질수록 긍정 클래스로 분류하는 경우가 늘어나면서 FPR이 증가한다. 모델의 정밀도가 떨어지는 것과 동일한 상황이다. 또한 모델이 긍정 클래스로 판단한 데이터의 비율이 증가하므로, FPR의 증가와 동시에 TPR 또한 증가하게 된다. 재현율이 높아지는 것이다. 따라서 정밀도-재현율 곡선을 통해 분류 임계값을 정했던 것처럼, ROC 커브를 이용해도 분류 임계값을 설정할 수 있다. 지표가 TPR과 FPR로 달라졌을 뿐이지, 정밀도-재현율 곡선과 완전히 동일한 맥락으로 이해할 수 있다.

앞에서 정밀도-재현율 그래프를 이용해, 발병 가능성 판단 모델이 높은 재현율에 더욱 신경쓰도록 분류 임계값을 정했다. 동일한 상황을 ROC 곡선에서 판단한다면, FPR이 다소 높아져도 매우 높은 수준의 TPR을 보장하도록 하는 분류 임계값을 설정하면 될 것이다.

다음 그림에서 초록색 점은 분류 임계값이 0.2인 점이고, 빨간 점은 분류 임계값이 0.8인 점이다. 분류 임계값이 낮아지면 긍정으로 분류하는 샘플의 수가 많아져 TPR과 FPR이 동시에 증가한다. 대부분의 샘플을 긍정으로 분류하게 되어 TP와 FP의 갯수가 동시에 늘어나기 때문이다. 정밀도-재현율 그래프를 볼 때와 같이 문제 상황에 맞는 TPR과 FPR의 판단 기준을 설정해서 분류 임계값을 설정해야 한다.

<center><img src="/assets/posts/images/UDL17/figure12.png" width=500></center>

#### AUC와 AP
정밀도-재현율 곡선과 ROC 커브를 이용해 모든 분류 임계값 지점에서 모델의 성능을 비교할 수 있게 되었다. 그렇다면 두 개의 이중 분류 모델 성능을 비교하려면 어떻게 해야 할까? 두 모델이 학습된 경향을 직접 파악할 수 없기 때문에, 동일한 분류 임계값을 설정한 후 정밀도와 재현율을 따지는 것은 의미가 없다. 어떤 모델은 임계값 0.9에서 최고의 성능이 나올 수도 있고, 다른 모델은 임계값 0.1이 최적일 수도 있기 때문이다. 그렇다면 다음으로, 그래프에서 둘 중 하나의 지표를 동일한 값으로 고정시키고 나머지 하나의 지표를 비교하는 방법을 생각해볼 수 있다. 반드시 동일한 평가 데이터셋을 이용해 두 개의 혼동 행렬을 구해야 한다. 다음과 같이 재현율 또는 TPR을 하나의 값으로 고정했을 때, 정밀도 또는 FPR의 차이를 보는 방식으로 모델의 성능을 비교할 수 있다.

<center><img src="/assets/posts/images/UDL17/figure15.png" width=500></center>

같은 평가 데이터셋에 대해 두 모델의 ROC 곡선을 그렸다. FPR이 동일하게 0.1로 측정될 때, 모델 1의 TPR이 더 높게 측정된다. 따라서 FPR이 0.1인 경우에는 모델 1이 더 성능이 좋다고 판단할 수 있다.

문제 상황에서 각 지표에 대한 목표 수치가 정해져 있으면 그것을 토대로 앞과 같이 성능을 평가하는 것이 제일 바람직하다. 그러나 아직 각 지표의 정확한 목표 수치를 정하지 못한 상태에서 여러 모델의 성능을 비교해야 한다면, 모든 임계값에서의 평균 성능을 구해보는 것이 제일 바람직할 것이다. 이를 위해 등장한 개념이 곡선의 __AUC(Area Under Curve)__ 이다.

AUC는 말 그대로 그래프에서 곡선 밑 영역의 넓이를 지칭한다. ROC 곡선을 떠올려보자. FPR이 0에 가깝게 변화해도 TPR이 높게 유지될수록 좋은 분류 모델이다. 긍정 클래스를 최대한 많이 잡아내는 동시에 정확히 분류해낸다는 뜻이기 때문이다. 따라서 다음의 그림과 같이. ROC 곡선이 좌상단에 가까이 붙을수록 분류 모델의 평균적인 성능이 좋은 것으로 해석할 수 있다.

<center><img src="/assets/posts/images/UDL17/figure17.png" width=700></center>

우측의 ROC 곡선을 보이는 모델은 어느 위치의 임계값을 사용해도 TPR이 높으면서 FPR이 작은 지점을 찾을 수 없다. 반면 좌측의 ROC 곡선을 보이는 모델은 좌상단에 가깝게 위치한 점의 임계값을 사용하면 매우 높은 성능의 분류기를 얻어낼 수 있다. 다시 말해서, 그래프에서 ROC 곡선 밑의 넓이가 넓을수록 모델의 평균적인 성능이 더 좋다는 이야기이고, 최종적으로 더 좋은 이중 분류기를 얻어낼 수 있다. 그래서 ROC의 AUC 값으로 모델의 성능을 비교하는 것이다.

<center><img src="/assets/posts/images/UDL17/figure16.png" width=700></center>

ROC 커브와 동일하게 정밀도-재현율 곡선에서도 AUC를 구할 수 있다. 정밀도-재현율 곡선 또한 우상단으로 곡선이 붙을 수록 평균적으로 좋은 성능을 가지는 모델이라고 이해할 수 있다. 정밀도-재현율 곡선의 AUC는 __AP(Average Precision)__ 이라는 특별한 이름으로 부른다. 모든 지점에서 정밀도의 평균이라는 뜻이다. ROC 곡선의 AUC와 같이, 정밀도-재현율 곡선이 가지는 AP의 값이 클수록 좋은 성능의 모델이다.

<center><img src="/assets/posts/images/UDL17/figure18.png" width=700></center>

ROC 곡선과 정밀도-재현율 곡선 기반의 성능 평가는 이중 분류 문제뿐만 아니라 다양한 태스크에서 사용된다. 대표적인 것이 물체 검출(Object Detection) 분야이다. 이후 챕터에서 물체 검출 문제를 다루면서 다시 보게 되니, 어떤 원리로 그래프가 그려지는지 확실하게 이해하고 가도록 하자.

## 3.8.2. 다중 분류 문제의 평가 지표
이중 분류 문제의 평가 지표에 이어, 이번에는 __다중 분류 문제의 평가 지표__ 를 알아보자. 앞서 다중 분류 문제를 푸는 모델을 학습할 때, 여러 개의 이중 분류기를 동시에 이용한다는 방식으로 접근했다. 따라서 평가 지표 또한 이중 분류 모델의 평가 지표를 다중 분류 문제에 맞춰 확장해서 사용한다.

### 다중 분류 모델의 혼동 행렬(Confusion Matrix)
앞선 챕터에서 구현했던 음식 분류 모델을 다시 떠올려보자. 어떤 음식의 영양소 데이터를 입력으로 받아 과자, 육류, 음료수, 디저트의 네 가지 클래스로 음식을 구분해낸다. 모델은 소프트맥스 활성 함수를 거쳐 각 클래스에 대한 __신뢰도__ 값들이 담긴 벡터를 출력한다. 출력 벡터는 원-핫 인코딩되어 각각 클래스에 대한 이중 분류 결과를 모아놓은 것과 같은 모양이 된다. 모델의 분류 결과 음료수 클래스의 위치에서 1이 출력되었을 경우, 모델의 출력을 다음과 같이 해석할 수 있다.

<center><img src="/assets/posts/images/UDL17/figure19.png" width=700></center>

- 입력 데이터의 음식이 __육류__ 인지 아닌지 분류 - __부정__
- 입력 데이터의 음식이 __과자__ 인지 아닌지 분류 - __긍정__
- 입력 데이터의 음식이 __음료수__ 인지 아닌지 분류 - __부정__
- 입력 데이터의 음식이 __디저트__ 인지 아닌지 분류 - __부정__

각각의 클래스에 대한 이중 분류 결과를 동시에 얻어낸 것과 동일하다. 따라서 이중 분류 모델의 혼동 행렬을 구한 것처럼, 각각의 클래스에 대한 모델의 혼동 행렬을 구해낼 수 있다. 이중 분류 문제에서의 혼동 행렬을 클래스 갯수만큼 확장한 형태이다. 이중 분류 문제에서는 '긍정'과 '부정'의 두 가지 클래스만을 가지고 2x2 모양의 행렬을 만들었다면, 다중 분류 문제에서는 이를 확장해 $$n$$개의 클래스에 대해 모델의 분류 결과를 $$n \times n$$ 모양의 행렬로 정리한다.

평가 데이터셋에서 필자가 학습한 모델의 분류 결과를 혼동 행렬로 정리하면 다음과 같다.

<center><img src="/assets/posts/images/UDL17//figure20.png" width=500></center>

세로축은 정답 클래스, 가로축은 모델이 분류한 클래스를 각각 의미한다. 각각의 칸에는 세로축에서 해당하는 정답 클래스의 데이터를 가로축에서 해당하는 클래스로 분류한 갯수를 표기한다. 예를 들어 앞의 혼동 행렬을 보면, 우리 모델이 음료수 데이터를 음료수로 정확히 분류한 갯수는 181개이고, 실제로는 디저트 데이터인데 음료수라고 잘못 분류한 갯수는 24개이다.

또한 정리한 혼동 행렬을 가지고 각 클래스별 TP, TN, FP, FN 샘플의 갯수를 구해볼 수 있다. '디저트' 클래스를 예로 들어 보자. 

<center><img src="/assets/posts/images/UDL17//figure21.png" width=500></center>

먼저 TP는, 디저트 클래스인 데이터를 디저트로 옳게 분류한 갯수이다. TP는 위 그림에서 초록색 셀에 해당하는 154개이다.

다음으로 디저트 클래스에 대한 TN는 디저트가 아닌 클래스의 데이터를 디저트 이외의 다른 클래스로 분류한 경우의 갯수이다. 디저트 클래스에 대한 TN이기 때문에, 해당 분류가 정확한지 아닌지는 상관없이 디저트 이외의 클래스로만 분류하면 TN 샘플이 된다. TN의 총 갯수는 위 그림에서 흰색 셀들의 총합인 578개이다.

디저트 클래스에 대한 FP는 디저트가 아닌 클래스의 데이터를 디저트 클래스로 잘못 분류한 경우의 갯수이다. FP는 위 그림에서 주황색 셀들의 총합인 22개이다.

마지막으로 디저트 클래스에 대한 FN는 디저트 클래스의 데이터를 디저트 이외의 다른 클래스들로 잘못 분류한 경우의 갯수이다. FN는 위 그림에서 노랑색 셀들의 총합인 46개이다.

위와 같은 방법으로 나머지 세 개의 클래스에 대해서도 분류 결과를 정리할 수 있다. 각 클래스에 대한 TP, TN, FP, FN 값을 구하면, 자연스럽게 클래스 별로 이중 분류 문제에서와 동일한 평가 지표를 계산해볼 수 있다. 다중 분류 문제의 평가 지표는 이중 분류 문제에서의 평가 지표를 기반으로 확장되어 정의된다.

### 관련 지표

#### 클래스별 분류 정확도와 전체 평균 분류 정확도
가장 먼저 __클래스별 분류 정확도__ 를 계산해볼 수 있다. 앞서 분류 정확도를 아래와 같이 정의하였다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BAccuracy%7D%20%3D%20%5Cfrac%7B%5Ctext%7BTP%7D%20&plus;%20%5Ctext%7BTN%7D%7D%7B%5Ctext%7BTP%7D%20&plus;%20%5Ctext%7BFP%7D%20&plus;%20%5Ctext%7BTN%7D%20&plus;%20%5Ctext%7BFN%7D%7D" width=300></center>

디저트 클래스에 대해 구한 TP, TN, FP, FN의 갯수를 대입하면, 디저트 클래스에 대한 분류 정확도를 계산할 수 있다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BAccuracy%7D%20%3D%20%5Cfrac%7B154%20&plus;%20578%7D%7B154%20&plus;%2046%20&plus;%20578%20&plus;%2022%7D%20%3D%200.915" width=375></center>

다른 클래스들도 같은 방법으로 분류 정확도를 계산해볼 수 있다. 모든 클래스에 대해 계산된 분류 정확도를 평균내면 __전체 평균 분류 정확도__ 가 된다.

<center><img src="/assets/posts/images/UDL17/figure22.png" width=200></center>

학습한 다중 분류 모델의 평균적인 성능을 보기 위해서는 당연히 전체 평균 분류 정확도를 확인해야 한다. 그러나 특정 클래스에 대한 정확도가 중요할 경우 해당 클래스의 정확도를 중점으로 성능을 판단해야 한다.

#### 클래스별 정밀도 및 전체 평균 정밀도
모델의 분류 결과를 혼동 행렬로 정리하고 각 클래스에 대한 TP, TN, FP, FN의 갯수를 구했으므로, 각 클래스에 대한 재현율과 정밀도 등의 분류 모델 평가 지표를 계산할 수 있다.

먼저 클래스 별 정밀도와 전체 평균 정밀도를 구해보자. 앞서 정밀도(Precision)를 다음과 같이 정의하였다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BPrecision%7D%20%3D%20%5Cfrac%7B%5Ctext%7BTP%7D%7D%7B%5Ctext%7BTP%7D%20&plus;%20%5Ctext%7BFP%7D%7D" width=175></center>

육류 클래스에 대한 분류 결과의 정밀도를 다음과 같이 계산할 수 있다. 

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BPrecision%7D%20%3D%20%5Cfrac%7B184%7D%7B184%20&plus;%2030%7D%20%3D%200.86" width=250></center>

이와 같은 방법으로 육류 클래스가 아닌 다른 클래스들에 대해서도 정밀도를 구할 수 있다. 또한 그렇게 구한 클래스 별 정밀도를 평균내면 평가 데이터셋 전체에 대한 모델의 평균 정밀도가 된다.

<center><img src="/assets/posts/images/UDL17/figure23.png" width=200></center>

각 클래스별 정밀도를 살펴보면, 과자 클래스의 정밀도가 특히 높고 다른 클래스들은 비슷한 수준의 정밀도를 보여주고 있다. 또한 전체 평균 정밀도는 평균 분류 정확도보다 더욱 설득력 있는 평가 지표이다. 다중 분류 문제의 특성상, 모델이 어느정도 학습된 후부터는 TN의 수가 상대적으로 매우 커지기 때문이다. 각각의 클래스에 대한 TN 샘플이 늘어나면, 그 샘플이 정확하게 분류되었는지 여부에 상관없이 분류 정확도는 높아진다. 때문에 각 클래스로 분류한 결과가 정확한지 여부를 자세히 판단할 지표로 정밀도를 사용한다.

#### 클래스별 재현율 및 전체 평균 재현율
다음으로 볼 수 있는 지표는 클래스별 재현율과 전체 평균 재현율이다. 앞서 재현율(Recall)을 다음과 같이 정의하였다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BRecall%7D%20%3D%20%5Cfrac%7B%5Ctext%7BTP%7D%7D%7B%5Ctext%7BTP%7D%20&plus;%20%5Ctext%7BFN%7D%7D" width=150></center>

육류 클래스에 대한 재현율을 다음과 같이 계산할 수 있다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20%5Ctext%7BRecall%7D%20%3D%20%5Cfrac%7B184%7D%7B184%20&plus;%2030%7D%20%3D%200.86" width=200></center>

이와 같은 방법으로 다른 클래스들에 대해서도 재현율을 구할 수 있다. 또한 그렇게 구한 클래스 별 재현율을 평균내면 평가 데이터셋 전체에 대한 모델의 평균 재현율이 된다.

<center><img src="/assets/posts/images/UDL17/figure24.png" width=200></center>

각 클래스별 재현율을 살펴보면, 디저트 클래스의 정밀도가 특히 낮게 나타난다. 디저트 클래스의 분류 정확도와 정밀도 지표가 각각 다른 클래스들과 비슷했던 것과는 대조적이다. 정확도와 정밀도가 상대적으로 높은 것에 비해 재현율이 낮은 것으로 보아, 디저트 클래스를 찾아낸다면 비교적 정확하게 찾아내지만 찾아내는 능력 자체는 다른 클래스에 비해 상대적으로 부족하다고 해석할 수 있다. 평균 재현율을 올리기 위해서는 디저트 클래스의 학습 데이터를 보강하는 방향으로 진행하는 것이 좋아 보인다.

앞에서 살펴본 바와 같이, 분류 결과에 대한 클래스별 정밀도와 재현율 지표를 이용해 다중 분류 모델의 성능 평가를 더욱 정밀하게 진행할 수 있다. 모델이 어떤 클래스에 대한 검출력과 구분력이 약한지 확실하게 판단할 수 있는 지표가 되기 때문이다.

#### 클래스별 F1 Score 및 전체 평균 F1 Score
앞에서 살펴본 바와 같이, 분류 결과에 대한 클래스별 재현율과 정밀도 지표를 이용해 다중 분류 모델의 성능 평가를 더욱 정밀하게 진행할 수 있다. 모델이 어떤 클래스에 대한 검출력과 구분력이 약한지 확실하게 판단할 수 있는 지표가 되기 때문이다.

그리고 클래스별로 재현율과 정밀도 지표가 정의되었으니, 클래스별 F1 점수와 전체 평균 F1 점수를 계산할 수 있다. 알다시피 F1 점수는 재현율과 정밀도 지표의 조화 평균으로, 모델의 전체적인 성능을 측정하기 위한 지표로 활용할 수 있다. F1 점수가 높아진다는 것은 재현율과 정밀도가 동시에 높아짐을 의미한다. 그래서 앞의 디저트 클래스 재현율과 같이 특히 성능이 좋지 않은 클래스의 문제가 해결된 후에, 모델의 전체적인 성능의 향상 정도를 평가하기 위해 활용하면 좋다.

앞서 F1 점수를 다음과 같이 정의하였다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20F_1%20%3D%202%20%5Ccdot%20%5Cfrac%7B%5Ctext%7BRecall%7D%20%5Ccdot%20%5Ctext%7BPrecision%7D%7D%7B%5Ctext%7BRecall%7D%20&plus;%20%5Ctext%7BPrecision%7D%7D" width=250></center>

일례로 육류 클래스에 대한 F1 점수를 다음과 같이 계산할 수 있다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20F_1%20%3D%202%20%5Ccdot%20%5Cfrac%7B0.92%20%5Ccdot%200.86%7D%7B0.92%20&plus;%200.86%7D%20%3D%200.88" width=250></center>

다른 클래스들에 대해서도 동일한 방법으로 F1 점수를 계산할 수 있고, 그것들을 평균내면 전체 데이터셋에 대한  모델의 평균 F1 점수가 된다. 모델의 전체적인 성능을 아우르는 지표로써 생각할 수 있다.

<center><img src="/assets/posts/images/UDL17/figure25.png" width=200></center>

#### Top-N 정확도
앞서 다중 분류 모델의 평가 지표를 혼동 행렬 기반으로 확장하였다. 학습과 검증시에는 모델의 전체적인 성능 향상을 위해 혼동 행렬 기반의 재현율과 정밀도 등의 평가 지표를 활용하는 것이 좋다. 그러나 학습을 마치고 모델의 최종 성능을 평가할 때는 __Top-N__ 정확도라는 또 다른 평가 지표를 사용할 수 있다.

우리가 계속 이야기했던 모델의 분류 정확도는 __Top-1 정확도__ 라고 바꾸어 부를 수 있다. 모델이 __신뢰도를 가장 높게 잡은 1개의 클래스__ 안에 정답 클래스가 있는 경우의 비율이라는 의미이다. 모델이 출력한 신뢰도 벡터를 원-핫 인코딩하기 전에, 가장 신뢰도를 높게 측정했던 클래스 하나가 정답 클래스인 경우이다. 지금까지 계산했던 분류 정확도는 무조건 Top-1 정확도였다. 모델이 출력하는 신뢰도 벡터를 원-핫 인코딩하면 가장 신뢰도가 큰 값만 1로 살아남기 때문이다. 원-핫 벡터에서 1로 설정된 클래스가 정답 클래스와 동일한지만을 판단했었다.

그러나 Top-N 정확도에서 N을 2 이상으로 올리면, 원-핫 인코딩을 하기 전의 신뢰도 벡터를 이용한다. 모델이 __신뢰도를 가장 높게 잡은 n개의 클래스__ 안에 정답 클래스가 있는 경우의 비율로 확장되기 때문에, 원-핫 벡터로는 결과를 알 수 없기 때문이다. 

앞서 학습했던 MNIST 데이터셋의 손글씨 숫자 분류를 구현한 모델을 Top-1과 Top-3 정확도로 평가하고 결과를 비교해보자. 필자가 임의로 학습한 손글씨 숫자 분류 모델에 데이터를 입력했다.

<center><img src="/assets/posts/images/UDL17/figure26.png" width=400></center>

출력 벡터는 위에서 아래로 0에서 9까지 클래스별 모델이 예측한 신뢰도 값이다. 입력한 사진 데이터의 정답 클래스는 숫자 3이므로, 출력 벡터에서 4번째 원소인 숫자 3 클래스의 신뢰도 값이 가장 커야 한다. 그러나 모델은 숫자 8 클래스의 신뢰도를 가장 높게 측정했으므로, Top-1 분류 결과로써는 분류에 실패한 것으로 판단한다.

그러나 Top-3 분류 결과는 다르다. 다음의 그림을 보자.

<center><img src="/assets/posts/images/UDL17/figure27.png" width=400></center>

Top-3 분류 결과는, 신뢰도 값을 가장 높게 판단한 세 개의 클래스 내에 정답 클래스가 있으면 분류에 성공한 것으로 본다. 앞의 그림에서 신뢰도 값을 가장 높게 판단한 상위 세 개의 클래스는 숫자 8 클래스 말고도 숫자 3과 숫자 5가 존재한다. 세 개의 클래스들 중 정답 클래스인 숫자 3이 존재하므로, Top-3 분류 결과에서는 이 경우를 분류에 성공한 것으로 본다.

앞에서 본 바와 같이, 모델이 분류한 클래스의 정답 여부를 Top-1 분류 결과로 보면 __Top-1 정확도__ 이고, Top-3 분류 결과로 보면 __Top-3 정확도__ 이다. 그리고 Top-N 정확도에서 N은 설정하기 나름으로 필요에 따라 얼마든지 커질 수 있다.

Top-N 분류 결과에서 N이 커지면 전체적인 정확도도 당연히 높아질 것이다. 상위 N개의 신뢰도를 가지는 클래스 내에 정답 클래스가 있을 확률이 커지기 때문이다. 학습한 손글씨 분류 모델의 Top-1 분류 정확도와 Top-3 분류 정확도를 구해 보면, 다음과 같이 예상한 결과를 보인다.

```bash
Top-1 Accuracy: 0.9012
Top-2 Accuracy: 0.961
Top-3 Accuracy: 0.9783
```

N이 커질수록 상위 N개 클래스에 정답 클래스가 위치할 가능성이 커지기 때문에 자연스럽게 정확도는 상승한다.

학습한 분류 모델을 실제로 사용할 때는 Top-1만이 의미가 있을 텐데 Top-N 정확도를 평가에 사용하는 이유는 무엇일까? 우리가 모델을 학습했던 4개 클래스의 음식 분류 문제나 10개 클래스의 손글씨 숫자 분류 문제에서는 실제로 큰 의미가 없다. 모델이 출력하는 신뢰도 벡터의 크기가 작기 때문에, Top-1 정확도만을 가지고도 모델의 성능을 명확히 파악할 수 있기 때문이다. 그러나 분류할 클래스가 수백 개, 수천 개 이상으로 많아지면, Top-1 분류 결과의 신뢰도가 점점 떨어지고 Top-N 분류 결과와의 유의미한 차이를 찾기 힘들게 된다. 단순히 생각해봐도, 100개의 클래스들 중 하나를 고르는 것과 10,000개의 클래스들 중 하나를 고르는 것의 난이도 차이는 매우 클 것으로 예상할 수 있다. 

어찌됐던 모든 클래스에 대한 신뢰도의 총합은 1로 고정이기 때문에, 클래스의 갯수가 많아질수록 Top-1 분류 결과의 정확도는 떨어질 수밖에 없다. 그래서 적당히 큰 갯수의 N을 설정해서, '그래도 모델이 수많은 클래스들 중 N개로 추려낼 능력은 이 정도 된다' 라는 것을 보기 위해 Top-N 정확도를 사용한다. 이후에도 같이 살펴보겠지만 ImageNet 등의 유명한 이미지 분류 문제들이 분류할 클래스의 갯수가 많아 Top-5 정확도를 활용하는 대표적인 문제들이다.

