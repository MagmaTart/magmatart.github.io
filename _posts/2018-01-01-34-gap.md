---
layout: article
title:  "Global Average Pooling (GAP)"
date:   2018-01-01 12:00:00 -0400
modify_date: 2018-01-01 12:00:00 -0400
tags:
- Machine Learning
- Deep Learning
category: 
- Development
use_math: true
---

DCGAN 논문을 보다가 Global Average Pooling이라는 기술에 대한 언급이 되어 있길래 정리해봅니다.

<!--more-->
-----
GAP가 처음으로 제안된 논문은 아래 논문입니다.

[Network In Network](https://arxiv.org/pdf/1312.4400.pdf)

기존의 CNN 구조에서는 Convolution과 Pooling Layer들을 거치면서 특징을 추출하고, 네트워크의 끝에 Fully-Connected Layer를 사용하여 분류를 수행합니다. 하지만 FC Layer의 가장 큰 문제점은, 네트워크의 크기가 클수록 Overfitting이 일어나기 쉽다는 점입니다. FC 레이어의 너비가 넓어질수록 Parameter가 많아져서 Overfitting이 일어나기 더 쉬운 환경이 만들어집니다.
 
그래서 __Global Average Pooling__ 이 제안되었습니다. GAP는 분류를 위해서 네트워크의 끝에서 클래스 개수 크기의 벡터를 만들어줄 때 FC 레이어를 사용하지 않아도 되게 만들어줍니다. 따라서 Parameter의 수가 크게 감소하고, Overfitting을 어느정도 방지해 주는 이점이 있습니다.

GAP는 간단한 아이디어입니다. CNN 구조를 거치고 나온 최종 Feature Map에 1 x 1 Convolution을 수행해서, __필터의 Depth가 분류 클래스의 개수와 맞게 맞춰줍니다.__ 그리고 각각의 필터를 한 장 단위로 Average Pooling합니다. 그림으로 나타내면 다음과 같은 모양이 됩니다.

<p align="center"><image width="400" src="/assets/posts/images/34/figure1.png"/></p>

1 x 1 Convolution은 출력 벡터의 원소 개수와 클래스의 개수를 동일하게 맞추기 위해서 사용됩니다.  2차원의 필터 한 장 당 하나의 값이 출력되기 때문에, 1 x 1 Convolution을 사용해서 필터의 장 수를 맞춰주는 겁니다. 

이렇게 GAP를 통해 만들어진 벡터의 원소 개수는 클래스의 개수와 같으므로, 바로 SOFTMAX 분류를 시행할 수 있습니다.